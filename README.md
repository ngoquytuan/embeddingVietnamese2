T√¥i s·∫Ω t·∫°o cho b·∫°n m·ªôt d·ª± √°n ho√†n ch·ªânh ƒë·ªÉ ƒë√°nh gi√° embedding ti·∫øng Vi·ªát d·ª±a tr√™n thi·∫øt k·∫ø ƒë√£ cung c·∫•p v√† t·ªëi ∆∞u cho GPU. D∆∞·ªõi ƒë√¢y l√† c·∫•u tr√∫c d·ª± √°n chi ti·∫øt:

## 1. C·∫•u tr√∫c th∆∞ m·ª•c

```
vietnamese_embedding_evaluator/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ models.json
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ content.md
‚îÇ   ‚îî‚îÄ‚îÄ test_suite.json
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îî‚îÄ‚îÄ # N∆°i l∆∞u c√°c file JSON k·∫øt qu·∫£ v√† bi·ªÉu ƒë·ªì
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ embedding_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ visualizer.py
‚îú‚îÄ‚îÄ evaluate.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## 2. File requirements.txt

```txt
sentence-transformers>=2.2.2
torch>=2.0.0
torchvision
torchaudio
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
underthesea>=6.7.0
pyvi>=0.1.1
scikit-learn>=1.3.0
transformers>=4.30.0
accelerate>=0.20.0
pathlib
```

## 3. configs/models.json

```json
{
  "models": [
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    "sentence-transformers/distiluse-base-multilingual-cased",
    "sentence-transformers/LaBSE",
    "keepitreal/vietnamese-sbert",
    "VoVanPhuc/sup-SimCSE-VietNamese-phobert-base",
    "vinai/phobert-base-v2"
  ]
}
```

## 4. data/content.md

```markdown
# L·ªãch s·ª≠ v√† Ph√°t tri·ªÉn c·ªßa Tr√≠ tu·ªá Nh√¢n t·∫°o t·∫°i Vi·ªát Nam

## Kh√°i ni·ªám v·ªÅ Tr√≠ tu·ªá Nh√¢n t·∫°o

Tr√≠ tu·ªá nh√¢n t·∫°o (AI) l√† m·ªôt lƒ©nh v·ª±c c·ªßa khoa h·ªçc m√°y t√≠nh t·∫≠p trung v√†o vi·ªác t·∫°o ra c√°c c·ªó m√°y th√¥ng minh c√≥ kh·∫£ nƒÉng ho·∫°t ƒë·ªông v√† ph·∫£n ·ª©ng nh∆∞ con ng∆∞·ªùi. Thu·∫≠t ng·ªØ "Artificial Intelligence" ƒë∆∞·ª£c John McCarthy ƒë·∫∑t ra l·∫ßn ƒë·∫ßu ti√™n v√†o nƒÉm 1956 t·∫°i H·ªôi ngh·ªã Dartmouth.

AI bao g·ªìm nhi·ªÅu lƒ©nh v·ª±c con nh∆∞ h·ªçc m√°y, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, th·ªã gi√°c m√°y t√≠nh, v√† robotics. M·ª•c ti√™u cu·ªëi c√πng c·ªßa AI l√† t·∫°o ra nh·ªØng h·ªá th·ªëng c√≥ th·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• ƒë√≤i h·ªèi tr√≠ th√¥ng minh c·ªßa con ng∆∞·ªùi.

## Giai ƒëo·∫°n ƒë·∫ßu c·ªßa AI (1950-1970)

Giai ƒëo·∫°n ƒë·∫ßu c·ªßa AI, t·ª´ nh·ªØng nƒÉm 1950 ƒë·∫øn 1970, ƒë∆∞·ª£c g·ªçi l√† th·ªùi k·ª≥ c·ªßa "AI bi·ªÉu t∆∞·ª£ng" hay "GOFAI" (Good Old-Fashioned AI). C√°c nh√† nghi√™n c·ª©u tin r·∫±ng tr√≠ th√¥ng minh c·ªßa con ng∆∞·ªùi c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ ph·ªèng b·∫±ng c√°ch s·ª≠ d·ª•ng logic to√°n h·ªçc v√† c√°c quy t·∫Øc bi·ªÉu t∆∞·ª£ng r√µ r√†ng.

Trong giai ƒëo·∫°n n√†y, c√°c nh√† khoa h·ªçc t·∫≠p trung v√†o vi·ªác ph√°t tri·ªÉn c√°c h·ªá th·ªëng chuy√™n gia v√† c√°c thu·∫≠t to√°n t√¨m ki·∫øm. H·ªç tin r·∫±ng c√≥ th·ªÉ gi·∫£i quy·∫øt m·ªçi v·∫•n ƒë·ªÅ b·∫±ng c√°ch l·∫≠p tr√¨nh c√°c quy t·∫Øc logic m·ªôt c√°ch t∆∞·ªùng minh.

## M√πa ƒë√¥ng AI ƒë·∫ßu ti√™n (1970-1980)

V√†o gi·ªØa nh·ªØng nƒÉm 1970, lƒ©nh v·ª±c AI tr·∫£i qua "m√πa ƒë√¥ng AI" ƒë·∫ßu ti√™n do s·ª± c·∫Øt gi·∫£m t√†i tr·ª£ nghi√™n c·ª©u v√† s·ª± th·∫•t v·ªçng v·ªÅ ti·∫øn ƒë·ªô ch·∫≠m ch·∫°p. C√°c h·ªá th·ªëng AI th·ªùi ƒë√≥ kh√¥ng th·ªÉ x·ª≠ l√Ω ƒë∆∞·ª£c s·ª± kh√¥ng ch·∫Øc ch·∫Øn v√† t√≠nh m∆° h·ªì c·ªßa th·∫ø gi·ªõi th·ª±c.

Nh·ªØng h·∫°n ch·∫ø ch√≠nh bao g·ªìm: kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu h·∫°n ch·∫ø, thi·∫øu s·ª©c m·∫°nh t√≠nh to√°n, v√† vi·ªác kh√¥ng th·ªÉ h·ªçc h·ªèi t·ª´ kinh nghi·ªám. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn s·ª± gi·∫£m s√∫t ƒë√°ng k·ªÉ trong ƒë·∫ßu t∆∞ v√† nghi√™n c·ª©u AI.

## S·ª± tr·ªói d·∫≠y c·ªßa M·∫°ng n∆°-ron (1980-2000)

S·ª± tr·ªói d·∫≠y c·ªßa m·∫°ng n∆°-ron nh√¢n t·∫°o v√† h·ªçc m√°y v√†o nh·ªØng nƒÉm 1980 v√† 1990 ƒë√£ m·ªü ra m·ªôt k·ª∑ nguy√™n m·ªõi cho AI. Thay v√¨ l·∫≠p tr√¨nh c√°c quy t·∫Øc m·ªôt c√°ch r√µ r√†ng, c√°c h·ªá th·ªëng b·∫Øt ƒë·∫ßu c√≥ kh·∫£ nƒÉng h·ªçc h·ªèi t·ª´ d·ªØ li·ªáu.

C√°c thu·∫≠t to√°n nh∆∞ backpropagation ƒë∆∞·ª£c ph√°t tri·ªÉn, cho ph√©p hu·∫•n luy·ªán m·∫°ng n∆°-ron nhi·ªÅu l·ªõp. ƒêi·ªÅu n√†y t·∫°o n·ªÅn t·∫£ng cho nh·ªØng ƒë·ªôt ph√° sau n√†y trong lƒ©nh v·ª±c h·ªçc s√¢u.

## K·ª∑ nguy√™n H·ªçc s√¢u (2010-nay)

Ng√†y nay, h·ªçc s√¢u (Deep Learning), m·ªôt nh√°nh c·ªßa h·ªçc m√°y s·ª≠ d·ª•ng m·∫°ng n∆°-ron s√¢u, ƒë√£ t·∫°o ra nh·ªØng ƒë·ªôt ph√° ƒë√°ng kinh ng·∫°c trong nhi·ªÅu lƒ©nh v·ª±c. C√°c ·ª©ng d·ª•ng bao g·ªìm nh·∫≠n d·∫°ng h√¨nh ·∫£nh, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, xe t·ª± l√°i, v√† y h·ªçc ch√≠nh x√°c.

C√°c m√¥ h√¨nh l·ªõn nh∆∞ GPT-3, GPT-4, BERT, v√† transformer architecture ƒë√£ c√°ch m·∫°ng h√≥a c√°ch ch√∫ng ta ti·∫øp c·∫≠n c√°c b√†i to√°n AI. S·ª©c m·∫°nh t√≠nh to√°n ng√†y c√†ng tƒÉng v√† l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì ƒë√£ t·∫°o ƒëi·ªÅu ki·ªán cho nh·ªØng ti·∫øn b·ªô n√†y.

## AI t·∫°i Vi·ªát Nam

Vi·ªát Nam ƒëang nhanh ch√≥ng ph√°t tri·ªÉn trong lƒ©nh v·ª±c AI v·ªõi nhi·ªÅu startup c√¥ng ngh·ªá v√† trung t√¢m nghi√™n c·ª©u. C√°c tr∆∞·ªùng ƒë·∫°i h·ªçc h√†ng ƒë·∫ßu nh∆∞ ƒê·∫°i h·ªçc B√°ch Khoa H√† N·ªôi, ƒê·∫°i h·ªçc Qu·ªëc Gia TP.HCM ƒë√£ th√†nh l·∫≠p c√°c khoa v√† ph√≤ng lab chuy√™n v·ªÅ AI.

Ch√≠nh ph·ªß Vi·ªát Nam ƒë√£ ban h√†nh Chi·∫øn l∆∞·ª£c qu·ªëc gia v·ªÅ nghi√™n c·ª©u, ph√°t tri·ªÉn v√† ·ª©ng d·ª•ng tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫øn nƒÉm 2030. M·ª•c ti√™u l√† bi·∫øn Vi·ªát Nam tr·ªü th√†nh m·ªôt trong nh·ªØng n∆∞·ªõc d·∫´n ƒë·∫ßu ASEAN v·ªÅ AI.

## ·ª®ng d·ª•ng AI trong th·ª±c t·∫ø

AI ƒë√£ ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong nhi·ªÅu lƒ©nh v·ª±c t·∫°i Vi·ªát Nam nh∆∞ ng√¢n h√†ng, th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠, y t·∫ø, gi√°o d·ª•c v√† n√¥ng nghi·ªáp. C√°c chatbot th√¥ng minh, h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m, v√† ph√¢n t√≠ch d·ªØ li·ªáu kh√°ch h√†ng ƒë√£ tr·ªü n√™n ph·ªï bi·∫øn.

Trong y t·∫ø, AI ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ch·∫©n ƒëo√°n h√¨nh ·∫£nh y khoa, d·ª± ƒëo√°n d·ªãch b·ªánh, v√† ph√°t tri·ªÉn thu·ªëc m·ªõi. Trong n√¥ng nghi·ªáp, AI gi√∫p t·ªëi ∆∞u h√≥a vi·ªác t∆∞·ªõi ti√™u, d·ª± b√°o th·ªùi ti·∫øt, v√† qu·∫£n l√Ω c√¢y tr·ªìng.

## Th√°ch th·ª©c v√† T∆∞∆°ng lai

M·∫∑c d√π c√≥ nhi·ªÅu ti·∫øn b·ªô, AI v·∫´n ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu th√°ch th·ª©c nh∆∞ v·∫•n ƒë·ªÅ ƒë·∫°o ƒë·ª©c, bias trong d·ªØ li·ªáu, b·∫£o m·∫≠t th√¥ng tin, v√† t√°c ƒë·ªông ƒë·∫øn vi·ªác l√†m. Vi·ªát Nam c·∫ßn ph√°t tri·ªÉn khung ph√°p l√Ω ph√π h·ª£p v√† ƒë√†o t·∫°o nh√¢n l·ª±c ch·∫•t l∆∞·ª£ng cao.

T∆∞∆°ng lai c·ªßa AI t·∫°i Vi·ªát Nam r·∫•t tri·ªÉn v·ªçng v·ªõi s·ª± ƒë·∫ßu t∆∞ m·∫°nh m·∫Ω v√†o nghi√™n c·ª©u v√† ph√°t tri·ªÉn. M·ª•c ti√™u l√† t·∫°o ra nh·ªØng s·∫£n ph·∫©m AI "Make in Vietnam" c√≥ th·ªÉ c·∫°nh tranh tr√™n th·ªã tr∆∞·ªùng qu·ªëc t·∫ø.
```

## 5. data/test_suite.json

```json
[
  {
    "question": "Ai l√† ng∆∞·ªùi ƒë·∫ßu ti√™n ƒë·∫∑t ra thu·∫≠t ng·ªØ Tr√≠ tu·ªá nh√¢n t·∫°o?",
    "correct_chunk_id": "chunk_0"
  },
  {
    "question": "H·ªôi ngh·ªã Dartmouth di·ªÖn ra v√†o nƒÉm n√†o?",
    "correct_chunk_id": "chunk_0"
  },
  {
    "question": "AI bi·ªÉu t∆∞·ª£ng hay GOFAI l√† g√¨?",
    "correct_chunk_id": "chunk_1"
  },
  {
    "question": "Giai ƒëo·∫°n ƒë·∫ßu c·ªßa AI t·∫≠p trung v√†o nh·ªØng g√¨?",
    "correct_chunk_id": "chunk_1"
  },
  {
    "question": "M√πa ƒë√¥ng AI ƒë·∫ßu ti√™n x·∫£y ra khi n√†o?",
    "correct_chunk_id": "chunk_2"
  },
  {
    "question": "Nguy√™n nh√¢n ch√≠nh g√¢y ra m√πa ƒë√¥ng AI l√† g√¨?",
    "correct_chunk_id": "chunk_2"
  },
  {
    "question": "Thu·∫≠t to√°n backpropagation ƒë∆∞·ª£c ph√°t tri·ªÉn v√†o th·ªùi gian n√†o?",
    "correct_chunk_id": "chunk_3"
  },
  {
    "question": "M·∫°ng n∆°-ron nh√¢n t·∫°o tr·ªói d·∫≠y v√†o giai ƒëo·∫°n n√†o?",
    "correct_chunk_id": "chunk_3"
  },
  {
    "question": "H·ªçc s√¢u l√† g√¨ v√† ·ª©ng d·ª•ng trong nh·ªØng lƒ©nh v·ª±c n√†o?",
    "correct_chunk_id": "chunk_4"
  },
  {
    "question": "GPT-4 v√† BERT thu·ªôc v·ªÅ k·ª∑ nguy√™n n√†o c·ªßa AI?",
    "correct_chunk_id": "chunk_4"
  },
  {
    "question": "Nh·ªØng tr∆∞·ªùng ƒë·∫°i h·ªçc n√†o ·ªü Vi·ªát Nam nghi√™n c·ª©u v·ªÅ AI?",
    "correct_chunk_id": "chunk_5"
  },
  {
    "question": "Chi·∫øn l∆∞·ª£c qu·ªëc gia v·ªÅ AI c·ªßa Vi·ªát Nam c√≥ m·ª•c ti√™u g√¨?",
    "correct_chunk_id": "chunk_5"
  },
  {
    "question": "AI ƒë∆∞·ª£c ·ª©ng d·ª•ng trong nh·ªØng lƒ©nh v·ª±c n√†o t·∫°i Vi·ªát Nam?",
    "correct_chunk_id": "chunk_6"
  },
  {
    "question": "AI ƒë∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o trong y t·∫ø v√† n√¥ng nghi·ªáp?",
    "correct_chunk_id": "chunk_6"
  },
  {
    "question": "Nh·ªØng th√°ch th·ª©c ch√≠nh c·ªßa AI hi·ªán nay l√† g√¨?",
    "correct_chunk_id": "chunk_7"
  },
  {
    "question": "T∆∞∆°ng lai AI t·∫°i Vi·ªát Nam nh∆∞ th·∫ø n√†o?",
    "correct_chunk_id": "chunk_7"
  }
]
```

## 6. src/data_processor.py

```python
import os
import re
import json
from pathlib import Path
from pyvi import ViTokenizer
import underthesea
from typing import List, Dict

class DataProcessor:
    def __init__(self):
        self.chunk_size = 150  # S·ªë t·ª´ t·ªëi ƒëa trong m·ªôt chunk
        self.overlap = 30      # S·ªë t·ª´ overlap gi·ªØa c√°c chunk
    
    def clean_text(self, text: str) -> str:
        """L√†m s·∫°ch vƒÉn b·∫£n ti·∫øng Vi·ªát"""
        # X√≥a markdown headers
        text = re.sub(r'#+\s*', '', text)
        # X√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def segment_vietnamese_text(self, text: str) -> List[str]:
        """T√°ch t·ª´ ti·∫øng Vi·ªát"""
        try:
            # S·ª≠ d·ª•ng underthesea ƒë·ªÉ t√°ch c√¢u
            sentences = underthesea.sent_tokenize(text)
            # T√°ch t·ª´ cho m·ªói c√¢u
            segmented_sentences = []
            for sentence in sentences:
                segmented = ViTokenizer.tokenize(sentence)
                segmented_sentences.append(segmented)
            return segmented_sentences
        except Exception as e:
            print(f"L·ªói khi t√°ch t·ª´: {e}")
            return [text]
    
    def create_chunks(self, text: str) -> List[Dict[str, str]]:
        """Chia vƒÉn b·∫£n th√†nh c√°c chunks v·ªõi overlap"""
        cleaned_text = self.clean_text(text)
        sentences = self.segment_vietnamese_text(cleaned_text)
        
        chunks = []
        current_chunk_words = []
        chunk_counter = 0
        
        for sentence in sentences:
            words = sentence.split()
            
            # N·∫øu th√™m c√¢u n√†y v√†o chunk hi·ªán t·∫°i v∆∞·ª£t qu√° gi·ªõi h·∫°n
            if len(current_chunk_words) + len(words) > self.chunk_size:
                if current_chunk_words:
                    # T·∫°o chunk t·ª´ c√°c t·ª´ hi·ªán t·∫°i
                    chunk_text = ' '.join(current_chunk_words)
                    chunks.append({
                        'id': f'chunk_{chunk_counter}',
                        'text': chunk_text,
                        'word_count': len(current_chunk_words)
                    })
                    chunk_counter += 1
                    
                    # Gi·ªØ l·∫°i overlap t·ª´ chunk tr∆∞·ªõc
                    overlap_words = current_chunk_words[-self.overlap:] if len(current_chunk_words) > self.overlap else current_chunk_words
                    current_chunk_words = overlap_words + words
                else:
                    current_chunk_words = words
            else:
                current_chunk_words.extend(words)
        
        # Th√™m chunk cu·ªëi c√πng n·∫øu c√≥
        if current_chunk_words:
            chunk_text = ' '.join(current_chunk_words)
            chunks.append({
                'id': f'chunk_{chunk_counter}',
                'text': chunk_text,
                'word_count': len(current_chunk_words)
            })
        
        return chunks
    
    def load_and_process_content(self, file_path: str) -> List[Dict[str, str]]:
        """Load v√† x·ª≠ l√Ω file content"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunks = self.create_chunks(content)
            print(f"ƒê√£ t·∫°o {len(chunks)} chunks t·ª´ file {file_path}")
            
            # In th·ªëng k√™
            word_counts = [chunk['word_count'] for chunk in chunks]
            print(f"S·ªë t·ª´ trung b√¨nh m·ªói chunk: {sum(word_counts) / len(word_counts):.1f}")
            print(f"Chunk ng·∫Øn nh·∫•t: {min(word_counts)} t·ª´")
            print(f"Chunk d√†i nh·∫•t: {max(word_counts)} t·ª´")
            
            return chunks
            
        except FileNotFoundError:
            print(f"Kh√¥ng t√¨m th·∫•y file: {file_path}")
            return []
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω file: {e}")
            return []
    
    def load_test_suite(self, file_path: str) -> List[Dict]:
        """Load b·ªô test questions"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                test_suite = json.load(f)
            print(f"ƒê√£ load {len(test_suite)} c√¢u h·ªèi test")
            return test_suite
        except Exception as e:
            print(f"L·ªói khi load test suite: {e}")
            return []
    
    def save_chunks_info(self, chunks: List[Dict], output_path: str):
        """L∆∞u th√¥ng tin chunks ƒë·ªÉ debug"""
        chunks_info = {
            'total_chunks': len(chunks),
            'chunks': chunks
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks_info, f, ensure_ascii=False, indent=2)
        
        print(f"ƒê√£ l∆∞u th√¥ng tin chunks v√†o: {output_path}")
```

## 7. src/embedding_manager.py

```python
import os
import torch
import numpy as np
from sentence_transformers import SentenceTransformer, util
from typing import List, Tuple, Dict
import time

class EmbeddingManager:
    def __init__(self, model_name: str, cache_dir: str = "./model_cache"):
        """
        Kh·ªüi t·∫°o Embedding Manager v·ªõi t·ªëi ∆∞u GPU
        """
        self.model_name = model_name
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # Ki·ªÉm tra GPU
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if torch.cuda.is_available():
            print(f"S·ª≠ d·ª•ng GPU: {torch.cuda.get_device_name(0)}")
            print(f"VRAM kh·∫£ d·ª•ng: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            print("S·ª≠ d·ª•ng CPU")
        
        # Load model
        print(f"ƒêang t·∫£i model: {model_name}...")
        start_time = time.time()
        
        try:
            self.model = SentenceTransformer(
                model_name, 
                device=self.device,
                cache_folder=cache_dir
            )
            load_time = time.time() - start_time
            print(f"ƒê√£ t·∫£i model th√†nh c√¥ng trong {load_time:.2f}s")
            print(f"Embedding dimension: {self.model.get_sentence_embedding_dimension()}")
            
        except Exception as e:
            print(f"L·ªói khi t·∫£i model {model_name}: {e}")
            raise
    
    def get_model_info(self) -> Dict:
        """L·∫•y th√¥ng tin model"""
        return {
            'name': self.model_name,
            'dimension': self.model.get_sentence_embedding_dimension(),
            'device': self.device,
            'max_sequence_length': getattr(self.model, 'max_seq_length', 'Unknown')
        }
    
    def encode_texts(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> torch.Tensor:
        """
        Encode danh s√°ch texts th√†nh embeddings v·ªõi t·ªëi ∆∞u GPU
        """
        if not texts:
            return torch.empty(0, self.model.get_sentence_embedding_dimension())
        
        print(f"ƒêang encode {len(texts)} texts...")
        start_time = time.time()
        
        try:
            # T·ªëi ∆∞u batch size d·ª±a tr√™n VRAM
            if self.device == "cuda":
                available_memory = torch.cuda.get_device_properties(0).total_memory
                if available_memory < 4e9:  # < 4GB
                    batch_size = min(16, batch_size)
                elif available_memory < 8e9:  # < 8GB  
                    batch_size = min(32, batch_size)
                else:  # >= 8GB
                    batch_size = min(64, batch_size)
            
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                convert_to_tensor=True,
                device=self.device,
                normalize_embeddings=True  # Normalize ƒë·ªÉ t·ªëi ∆∞u cosine similarity
            )
            
            encode_time = time.time() - start_time
            print(f"Ho√†n th√†nh encoding trong {encode_time:.2f}s")
            print(f"Shape: {embeddings.shape}")
            
            return embeddings
            
        except Exception as e:
            print(f"L·ªói khi encode: {e}")
            raise
    
    def find_most_similar(self, query_embedding: torch.Tensor, 
                         corpus_embeddings: torch.Tensor, 
                         top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """
        T√¨m top-k embeddings t∆∞∆°ng ƒë·ªìng nh·∫•t v·ªõi query
        """
        try:
            # T√≠nh cosine similarity
            similarities = util.cos_sim(query_embedding, corpus_embeddings)
            
            # L·∫•y top-k results
            top_k = min(top_k, corpus_embeddings.shape[0])
            top_results = torch.topk(similarities, k=top_k, dim=-1)
            
            indices = top_results.indices.cpu().numpy().flatten()
            scores = top_results.values.cpu().numpy().flatten()
            
            return indices, scores
            
        except Exception as e:
            print(f"L·ªói khi t√¨m ki·∫øm t∆∞∆°ng ƒë·ªìng: {e}")
            raise
    
    def batch_search(self, queries: List[str], corpus_embeddings: torch.Tensor, 
                    chunk_ids: List[str], top_k: int = 5) -> List[Dict]:
        """
        Th·ª±c hi·ªán batch search cho nhi·ªÅu queries
        """
        print(f"ƒêang th·ª±c hi·ªán batch search cho {len(queries)} queries...")
        
        # Encode t·∫•t c·∫£ queries
        query_embeddings = self.encode_texts(queries, show_progress=False)
        
        results = []
        for i, (query, query_emb) in enumerate(zip(queries, query_embeddings)):
            indices, scores = self.find_most_similar(
                query_emb.unsqueeze(0), 
                corpus_embeddings, 
                top_k
            )
            
            top_chunks = []
            for idx, score in zip(indices, scores):
                top_chunks.append({
                    'chunk_id': chunk_ids[idx],
                    'score': float(score),
                    'rank': len(top_chunks) + 1
                })
            
            results.append({
                'query': query,
                'top_results': top_chunks
            })
        
        return results
    
    def clear_cache(self):
        """D·ªçn d·∫πp cache GPU"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            print("ƒê√£ d·ªçn d·∫πp GPU cache")
    
    def get_memory_usage(self) -> Dict:
        """L·∫•y th√¥ng tin s·ª≠ d·ª•ng memory"""
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1e9
            cached = torch.cuda.memory_reserved() / 1e9
            return {
                'allocated_gb': allocated,
                'cached_gb': cached,
                'device': torch.cuda.get_device_name(0)
            }
        else:
            return {'device': 'CPU', 'allocated_gb': 0, 'cached_gb': 0}
```

## 8. src/metrics.py

```python
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
from collections import defaultdict

class MetricsCalculator:
    def __init__(self):
        self.metrics_names = [
            'MRR', 'Hit_Rate@1', 'Hit_Rate@3', 'Hit_Rate@5', 
            'MAP@5', 'NDCG@5', 'Precision@5', 'Recall@5'
        ]
    
    def calculate_reciprocal_rank(self, correct_chunk_id: str, ranked_results: List[Dict]) -> float:
        """T√≠nh Reciprocal Rank cho m·ªôt query"""
        for rank, result in enumerate(ranked_results, 1):
            if result['chunk_id'] == correct_chunk_id:
                return 1.0 / rank
        return 0.0
    
    def calculate_hit_rate_at_k(self, correct_chunk_id: str, ranked_results: List[Dict], k: int) -> int:
        """T√≠nh Hit Rate@k cho m·ªôt query"""
        top_k_ids = [result['chunk_id'] for result in ranked_results[:k]]
        return 1 if correct_chunk_id in top_k_ids else 0
    
    def calculate_precision_at_k(self, correct_chunk_id: str, ranked_results: List[Dict], k: int) -> float:
        """T√≠nh Precision@k"""
        top_k_ids = [result['chunk_id'] for result in ranked_results[:k]]
        relevant_found = sum(1 for chunk_id in top_k_ids if chunk_id == correct_chunk_id)
        return relevant_found / min(k, len(ranked_results))
    
    def calculate_recall_at_k(self, correct_chunk_id: str, ranked_results: List[Dict], k: int) -> float:
        """T√≠nh Recall@k (trong tr∆∞·ªùng h·ª£p n√†y, m·ªói query ch·ªâ c√≥ 1 correct answer)"""
        top_k_ids = [result['chunk_id'] for result in ranked_results[:k]]
        return 1.0 if correct_chunk_id in top_k_ids else 0.0
    
    def calculate_average_precision(self, correct_chunk_id: str, ranked_results: List[Dict]) -> float:
        """T√≠nh Average Precision"""
        precision_at_k = []
        for k in range(1, len(ranked_results) + 1):
            if ranked_results[k-1]['chunk_id'] == correct_chunk_id:
                precision = self.calculate_precision_at_k(correct_chunk_id, ranked_results, k)
                precision_at_k.append(precision)
        
        return np.mean(precision_at_k) if precision_at_k else 0.0
    
    def calculate_ndcg_at_k(self, correct_chunk_id: str, ranked_results: List[Dict], k: int) -> float:
        """T√≠nh NDCG@k"""
        # T√≠nh DCG@k
        dcg = 0.0
        for i, result in enumerate(ranked_results[:k]):
            if result['chunk_id'] == correct_chunk_id:
                relevance = 1  # Binary relevance
                dcg += relevance / np.log2(i + 2)  # i+2 v√¨ log2(1) = 0
        
        # T√≠nh IDCG@k (trong tr∆∞·ªùng h·ª£p n√†y l√† 1.0 v√¨ ch·ªâ c√≥ 1 correct answer)
        idcg = 1.0  # 1 / log2(2) = 1
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def calculate_all_metrics(self, test_results: List[Dict]) -> Dict:
        """T√≠nh t·∫•t c·∫£ metrics cho to√†n b·ªô test results"""
        if not test_results:
            return {metric: 0.0 for metric in self.metrics_names}
        
        rr_scores = []
        hit_1_scores = []
        hit_3_scores = []
        hit_5_scores = []
        ap_scores = []
        ndcg_5_scores = []
        precision_5_scores = []
        recall_5_scores = []
        
        for result in test_results:
            correct_id = result['correct_chunk_id']
            ranked_results = result['top_5_results']
            
            # Reciprocal Rank
            rr = self.calculate_reciprocal_rank(correct_id, ranked_results)
            rr_scores.append(rr)
            
            # Hit Rates
            hit_1_scores.append(self.calculate_hit_rate_at_k(correct_id, ranked_results, 1))
            hit_3_scores.append(self.calculate_hit_rate_at_k(correct_id, ranked_results, 3))
            hit_5_scores.append(self.calculate_hit_rate_at_k(correct_id, ranked_results, 5))
            
            # Average Precision
            ap = self.calculate_average_precision(correct_id, ranked_results)
            ap_scores.append(ap)
            
            # NDCG@5
            ndcg_5 = self.calculate_ndcg_at_k(correct_id, ranked_results, 5)
            ndcg_5_scores.append(ndcg_5)
            
            # Precision@5 v√† Recall@5
            precision_5 = self.calculate_precision_at_k(correct_id, ranked_results, 5)
            recall_5 = self.calculate_recall_at_k(correct_id, ranked_results, 5)
            precision_5_scores.append(precision_5)
            recall_5_scores.append(recall_5)
        
        # T√≠nh mean cho t·∫•t c·∫£ metrics
        metrics = {
            'MRR': np.mean(rr_scores),
            'Hit_Rate@1': np.mean(hit_1_scores),
            'Hit_Rate@3': np.mean(hit_3_scores),
            'Hit_Rate@5': np.mean(hit_5_scores),
            'MAP@5': np.mean(ap_scores),
            'NDCG@5': np.mean(ndcg_5_scores),
            '
```python
            'Precision@5': np.mean(precision_5_scores),
            'Recall@5': np.mean(recall_5_scores)
        }
        
        return metrics
    
    def calculate_detailed_metrics(self, test_results: List[Dict]) -> Tuple[Dict, pd.DataFrame]:
        """T√≠nh metrics chi ti·∫øt cho t·ª´ng c√¢u h·ªèi"""
        detailed_results = []
        
        for i, result in enumerate(test_results):
            correct_id = result['correct_chunk_id']
            ranked_results = result['top_5_results']
            question = result['question']
            
            # T√¨m rank c·ªßa correct answer
            found_rank = None
            for rank, res in enumerate(ranked_results, 1):
                if res['chunk_id'] == correct_id:
                    found_rank = rank
                    break
            
            detailed_result = {
                'question_id': i + 1,
                'question': question[:50] + "..." if len(question) > 50 else question,
                'correct_chunk_id': correct_id,
                'found_rank': found_rank,
                'reciprocal_rank': self.calculate_reciprocal_rank(correct_id, ranked_results),
                'hit@1': self.calculate_hit_rate_at_k(correct_id, ranked_results, 1),
                'hit@3': self.calculate_hit_rate_at_k(correct_id, ranked_results, 3),
                'hit@5': self.calculate_hit_rate_at_k(correct_id, ranked_results, 5),
                'ap': self.calculate_average_precision(correct_id, ranked_results),
                'ndcg@5': self.calculate_ndcg_at_k(correct_id, ranked_results, 5),
                'top_1_score': ranked_results[0]['score'] if ranked_results else 0.0,
                'correct_score': next((res['score'] for res in ranked_results if res['chunk_id'] == correct_id), 0.0)
            }
            detailed_results.append(detailed_result)
        
        df_detailed = pd.DataFrame(detailed_results)
        summary_metrics = self.calculate_all_metrics(test_results)
        
        return summary_metrics, df_detailed
    
    def get_performance_analysis(self, df_detailed: pd.DataFrame) -> Dict:
        """Ph√¢n t√≠ch hi·ªáu su·∫•t chi ti·∫øt"""
        analysis = {
            'total_questions': len(df_detailed),
            'questions_answered_correctly_at_rank_1': int(df_detailed['hit@1'].sum()),
            'questions_answered_correctly_at_rank_3': int(df_detailed['hit@3'].sum()),
            'questions_answered_correctly_at_rank_5': int(df_detailed['hit@5'].sum()),
            'questions_not_found_in_top_5': int((df_detailed['found_rank'].isna()).sum()),
            'average_correct_answer_rank': df_detailed[df_detailed['found_rank'].notna()]['found_rank'].mean(),
            'median_correct_answer_rank': df_detailed[df_detailed['found_rank'].notna()]['found_rank'].median(),
            'score_statistics': {
                'top_1_score_mean': df_detailed['top_1_score'].mean(),
                'top_1_score_std': df_detailed['top_1_score'].std(),
                'correct_score_mean': df_detailed[df_detailed['correct_score'] > 0]['correct_score'].mean(),
                'correct_score_std': df_detailed[df_detailed['correct_score'] > 0]['correct_score'].std()
            }
        }
        
        return analysis
    
    def compare_models(self, model_results: Dict[str, Dict]) -> pd.DataFrame:
        """So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c models"""
        comparison_data = []
        
        for model_name, metrics in model_results.items():
            row = {'Model': model_name}
            row.update(metrics)
            comparison_data.append(row)
        
        df_comparison = pd.DataFrame(comparison_data)
        
        # S·∫Øp x·∫øp theo MRR (cao nh·∫•t tr∆∞·ªõc)
        df_comparison = df_comparison.sort_values('MRR', ascending=False).reset_index(drop=True)
        
        # Th√™m ranking cho t·ª´ng metric
        for metric in self.metrics_names:
            if metric in df_comparison.columns:
                df_comparison[f'{metric}_rank'] = df_comparison[metric].rank(ascending=False, method='min')
        
        return df_comparison
```

## 9. src/visualizer.py

```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List
import json

# C·∫•u h√¨nh matplotlib cho ti·∫øng Vi·ªát
plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']
sns.set_style("whitegrid")
sns.set_palette("husl")

class ResultVisualizer:
    def __init__(self, output_dir: str = "reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # C·∫•u h√¨nh m√†u s·∫Øc
        self.colors = sns.color_palette("husl", 10)
        
    def plot_model_comparison(self, df_comparison: pd.DataFrame, save_path: str = None) -> None:
        """V·∫Ω bi·ªÉu ƒë·ªì so s√°nh hi·ªáu su·∫•t c√°c models"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('So s√°nh Hi·ªáu su·∫•t c√°c Models Embedding Ti·∫øng Vi·ªát', fontsize=16, fontweight='bold')
        
        # MRR Comparison
        axes[0, 0].barh(df_comparison['Model'], df_comparison['MRR'], color=self.colors[0])
        axes[0, 0].set_title('Mean Reciprocal Rank (MRR)')
        axes[0, 0].set_xlabel('MRR Score')
        for i, v in enumerate(df_comparison['MRR']):
            axes[0, 0].text(v + 0.01, i, f'{v:.3f}', va='center')
        
        # Hit Rates Comparison
        hit_metrics = ['Hit_Rate@1', 'Hit_Rate@3', 'Hit_Rate@5']
        x = np.arange(len(df_comparison['Model']))
        width = 0.25
        
        for i, metric in enumerate(hit_metrics):
            if metric in df_comparison.columns:
                axes[0, 1].bar(x + i*width, df_comparison[metric], width, 
                              label=metric, color=self.colors[i+1])
        
        axes[0, 1].set_xlabel('Models')
        axes[0, 1].set_ylabel('Hit Rate')
        axes[0, 1].set_title('Hit Rates Comparison')
        axes[0, 1].set_xticks(x + width)
        axes[0, 1].set_xticklabels([name.split('/')[-1] for name in df_comparison['Model']], rotation=45)
        axes[0, 1].legend()
        
        # NDCG@5 v√† MAP@5
        if 'NDCG@5' in df_comparison.columns and 'MAP@5' in df_comparison.columns:
            x = np.arange(len(df_comparison['Model']))
            width = 0.35
            
            axes[1, 0].bar(x - width/2, df_comparison['NDCG@5'], width, 
                          label='NDCG@5', color=self.colors[4])
            axes[1, 0].bar(x + width/2, df_comparison['MAP@5'], width, 
                          label='MAP@5', color=self.colors[5])
            
            axes[1, 0].set_xlabel('Models')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].set_title('NDCG@5 v√† MAP@5 Comparison')
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels([name.split('/')[-1] for name in df_comparison['Model']], rotation=45)
            axes[1, 0].legend()
        
        # Overall Ranking
        ranking_metrics = ['MRR_rank', 'Hit_Rate@1_rank', 'Hit_Rate@3_rank', 'Hit_Rate@5_rank']
        available_rankings = [col for col in ranking_metrics if col in df_comparison.columns]
        
        if available_rankings:
            avg_rank = df_comparison[available_rankings].mean(axis=1)
            axes[1, 1].barh(df_comparison['Model'], avg_rank, color=self.colors[6])
            axes[1, 1].set_title('Average Ranking (Lower is Better)')
            axes[1, 1].set_xlabel('Average Rank')
            for i, v in enumerate(avg_rank):
                axes[1, 1].text(v + 0.1, i, f'{v:.1f}', va='center')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh models: {save_path}")
        
        plt.show()
    
    def plot_detailed_analysis(self, model_name: str, df_detailed: pd.DataFrame, 
                              metrics_summary: Dict, save_path: str = None) -> None:
        """V·∫Ω bi·ªÉu ƒë·ªì ph√¢n t√≠ch chi ti·∫øt cho m·ªôt model"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        model_short_name = model_name.split('/')[-1]
        fig.suptitle(f'Ph√¢n t√≠ch Chi ti·∫øt - {model_short_name}', fontsize=16, fontweight='bold')
        
        # 1. Distribution of Found Ranks
        ranks = df_detailed[df_detailed['found_rank'].notna()]['found_rank']
        if len(ranks) > 0:
            axes[0, 0].hist(ranks, bins=range(1, 7), alpha=0.7, color=self.colors[0])
            axes[0, 0].set_xlabel('Rank c·ªßa C√¢u tr·∫£ l·ªùi ƒê√∫ng')
            axes[0, 0].set_ylabel('S·ªë l∆∞·ª£ng C√¢u h·ªèi')
            axes[0, 0].set_title('Ph√¢n b·ªë Rank c·ªßa C√¢u tr·∫£ l·ªùi ƒê√∫ng')
            axes[0, 0].set_xticks(range(1, 6))
        
        # 2. Score Distribution
        correct_scores = df_detailed[df_detailed['correct_score'] > 0]['correct_score']
        top1_scores = df_detailed['top_1_score']
        
        axes[0, 1].hist([correct_scores, top1_scores], label=['Correct Answer Score', 'Top-1 Score'], 
                       alpha=0.7, color=self.colors[1:3])
        axes[0, 1].set_xlabel('Cosine Similarity Score')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title('Ph√¢n b·ªë ƒêi·ªÉm s·ªë')
        axes[0, 1].legend()
        
        # 3. Performance by Question Position
        question_performance = df_detailed.groupby(df_detailed.index // 4)['hit@1'].mean()  # Group every 4 questions
        axes[0, 2].plot(question_performance.index, question_performance.values, 
                       marker='o', color=self.colors[3])
        axes[0, 2].set_xlabel('Nh√≥m C√¢u h·ªèi (m·ªói nh√≥m 4 c√¢u)')
        axes[0, 2].set_ylabel('Hit Rate@1')
        axes[0, 2].set_title('Hi·ªáu su·∫•t theo Nh√≥m C√¢u h·ªèi')
        
        # 4. Metrics Overview
        metrics_names = ['MRR', 'Hit_Rate@1', 'Hit_Rate@3', 'Hit_Rate@5', 'MAP@5', 'NDCG@5']
        available_metrics = [m for m in metrics_names if m in metrics_summary]
        metric_values = [metrics_summary[m] for m in available_metrics]
        
        axes[1, 0].bar(available_metrics, metric_values, color=self.colors[4])
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].set_title('T·ªïng quan C√°c Ch·ªâ s·ªë')
        axes[1, 0].set_xticklabels(available_metrics, rotation=45)
        
        # Add value labels on bars
        for i, v in enumerate(metric_values):
            axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # 5. Question Difficulty Analysis
        df_detailed['difficulty'] = 'Easy'
        df_detailed.loc[df_detailed['found_rank'] > 3, 'difficulty'] = 'Hard'
        df_detailed.loc[df_detailed['found_rank'].isna(), 'difficulty'] = 'Very Hard'
        
        difficulty_counts = df_detailed['difficulty'].value_counts()
        axes[1, 1].pie(difficulty_counts.values, labels=difficulty_counts.index, 
                      autopct='%1.1f%%', colors=self.colors[5:8])
        axes[1, 1].set_title('Ph√¢n lo·∫°i ƒê·ªô kh√≥ C√¢u h·ªèi')
        
        # 6. Score Gap Analysis
        df_detailed['score_gap'] = df_detailed['top_1_score'] - df_detailed['correct_score']
        score_gaps = df_detailed[df_detailed['correct_score'] > 0]['score_gap']
        
        axes[1, 2].hist(score_gaps, bins=20, alpha=0.7, color=self.colors[8])
        axes[1, 2].set_xlabel('Score Gap (Top-1 - Correct)')
        axes[1, 2].set_ylabel('Frequency')
        axes[1, 2].set_title('Kho·∫£ng c√°ch ƒêi·ªÉm s·ªë')
        axes[1, 2].axvline(0, color='red', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì ph√¢n t√≠ch chi ti·∫øt: {save_path}")
        
        plt.show()
    
    def create_performance_heatmap(self, all_model_results: Dict, save_path: str = None) -> None:
        """T·∫°o heatmap so s√°nh hi·ªáu su·∫•t t·∫•t c·∫£ models"""
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        models = []
        metrics_data = []
        
        for model_name, result in all_model_results.items():
            models.append(model_name.split('/')[-1])
            metrics = result['summary_metrics']
            metrics_data.append([
                metrics.get('MRR', 0),
                metrics.get('Hit_Rate@1', 0),
                metrics.get('Hit_Rate@3', 0),
                metrics.get('Hit_Rate@5', 0),
                metrics.get('MAP@5', 0),
                metrics.get('NDCG@5', 0)
            ])
        
        df_heatmap = pd.DataFrame(
            metrics_data, 
            index=models,
            columns=['MRR', 'Hit@1', 'Hit@3', 'Hit@5', 'MAP@5', 'NDCG@5']
        )
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(df_heatmap, annot=True, cmap='YlOrRd', fmt='.3f', 
                   cbar_kws={'label': 'Performance Score'})
        plt.title('Performance Heatmap - All Models', fontsize=14, fontweight='bold')
        plt.ylabel('Models')
        plt.xlabel('Metrics')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ƒê√£ l∆∞u heatmap: {save_path}")
        
        plt.show()
    
    def generate_performance_report(self, all_model_results: Dict, 
                                   output_file: str = "performance_report.html") -> None:
        """T·∫°o b√°o c√°o HTML t·ªïng h·ª£p"""
        html_content = """
        <html>
        <head>
            <title>Vietnamese Embedding Models - Performance Report</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                h1, h2 { color: #2c3e50; }
                table { border-collapse: collapse; width: 100%; margin: 20px 0; }
                th, td { border: 1px solid #ddd; padding: 12px; text-align: center; }
                th { background-color: #f8f9fa; font-weight: bold; }
                .best { background-color: #d4edda; font-weight: bold; }
                .second { background-color: #e2f3ff; }
                .summary { background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin: 20px 0; }
            </style>
        </head>
        <body>
        """
        
        html_content += "<h1>Vietnamese Embedding Models - Performance Report</h1>"
        
        # T·∫°o b·∫£ng so s√°nh
        models_data = []
        for model_name, result in all_model_results.items():
            models_data.append({
                'Model': model_name.split('/')[-1],
                'Full_Name': model_name,
                'Dimension': result.get('embedding_dimension', 'N/A'),
                'Time': result.get('evaluation_time_seconds', 'N/A'),
                **result['summary_metrics']
            })
        
        df = pd.DataFrame(models_data)
        df = df.sort_values('MRR', ascending=False)
        
        html_content += "<h2>Model Performance Comparison</h2>"
        html_content += df.to_html(classes='performance-table', escape=False, index=False)
        
        # Top performer summary
        best_model = df.iloc[0]
        html_content += f"""
        <div class="summary">
            <h3>üèÜ Best Performing Model</h3>
            <p><strong>{best_model['Model']}</strong> achieved the highest MRR score of <strong>{best_model['MRR']:.4f}</strong></p>
            <ul>
                <li>Hit Rate@1: {best_model['Hit_Rate@1']:.2%}</li>
                <li>Hit Rate@5: {best_model['Hit_Rate@5']:.2%}</li>
                <li>Embedding Dimension: {best_model['Dimension']}</li>
                <li>Evaluation Time: {best_model['Time']:.2f}s</li>
            </ul>
        </div>
        """
        
        html_content += "</body></html>"
        
        output_path = self.output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ƒê√£ t·∫°o b√°o c√°o HTML: {output_path}")
```

## 10. evaluate.py (Script ch√≠nh)

```python
import os
import json
import pandas as pd
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import numpy as np
from pyvi import ViTokenizer
import underthesea
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import torch

from src.data_processor import DataProcessor
from src.embedding_manager import EmbeddingManager
from src.metrics import MetricsCalculator
from src.visualizer import ResultVisualizer

def setup_directories():
    """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    directories = ['reports', 'reports/visualizations', 'model_cache', 'logs']
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)

def load_configuration():
    """Load c·∫•u h√¨nh models v√† test suite"""
    print("üîß ƒêang load c·∫•u h√¨nh...")
    
    # Load models list
    try:
        with open('configs/models.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        models_to_test = config['models']
        print(f"‚úÖ ƒê√£ load {len(models_to_test)} models ƒë·ªÉ test")
    except Exception as e:
        print(f"‚ùå L·ªói khi load models config: {e}")
        return None, None
    
    # Load test suite
    try:
        with open('data/test_suite.json', 'r', encoding='utf-8') as f:
            test_suite = json.load(f)
        print(f"‚úÖ ƒê√£ load {len(test_suite)} c√¢u h·ªèi test")
    except Exception as e:
        print(f"‚ùå L·ªói khi load test suite: {e}")
        return None, None
    
    return models_to_test, test_suite

def evaluate_single_model(model_name: str, chunks: list, test_suite: list, 
                         chunk_texts: list, chunk_ids: list) -> dict:
    """ƒê√°nh gi√° m·ªôt model duy nh·∫•t"""
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° model: {model_name}")
    start_time = time.time()
    
    try:
        # Kh·ªüi t·∫°o embedding manager
        manager = EmbeddingManager(model_name)
        model_info = manager.get_model_info()
        
        # Encode corpus
        print("üìù ƒêang encode corpus...")
        corpus_embeddings = manager.encode_texts(chunk_texts)
        
        # Process test cases
        print("üîç ƒêang th·ª±c hi·ªán t√¨m ki·∫øm cho c√°c c√¢u h·ªèi test...")
        test_case_results = []
        
        for i, test_case in enumerate(test_suite):
            question = test_case['question']
            correct_chunk_id = test_case['correct_chunk_id']
            
            # Encode question
            query_embedding = manager.encode_texts([question], show_progress=False)
            
            # Find similar chunks
            top_indices, top_scores = manager.find_most_similar(
                query_embedding, corpus_embeddings, top_k=5
            )
            
            # Build results
            top_results = []
            for idx, score in zip(top_indices, top_scores):
                top_results.append({
                    'chunk_id': chunk_ids[idx],
                    'score': float(score),
                    'text_preview': chunks[idx]['text'][:100] + "..."
                })
            
            # Find rank of correct answer
            found_rank = None
            for rank, result in enumerate(top_results, 1):
                if result['chunk_id'] == correct_chunk_id:
                    found_rank = rank
                    break
            
            test_case_results.append({
                'question': question,
                'correct_chunk_id': correct_chunk_id,
                'found_rank': found_rank,
                'top_5_results': top_results
            })
            
            if (i + 1) % 5 == 0:
                print(f"  ƒê√£ x·ª≠ l√Ω {i + 1}/{len(test_suite)} c√¢u h·ªèi")
        
        # Calculate metrics
        print("üìä ƒêang t√≠nh to√°n metrics...")
        calculator = MetricsCalculator()
        summary_metrics, detailed_df = calculator.calculate_detailed_metrics(test_case_results)
        performance_analysis = calculator.get_performance_analysis(detailed_df)
        
        # Memory info
        memory_info = manager.get_memory_usage()
        
        # Clean up
        manager.clear_cache()
        del manager
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        evaluation_time = time.time() - start_time
        print(f"‚úÖ Ho√†n th√†nh ƒë√°nh gi√° {model_name} trong {evaluation_time:.2f}s")
        print(f"   MRR: {summary_metrics['MRR']:.4f}")
        print(f"   Hit Rate@1: {summary_metrics['Hit_Rate@1']:.2%}")
        
        return {
            'model_name': model_name,
            'model_info': model_info,
            'embedding_dimension': model_info['dimension'],
            'evaluation_time_seconds': evaluation_time,
            'summary_metrics': summary_metrics,
            'detailed_metrics': detailed_df.to_dict('records'),
            'performance_analysis': performance_analysis,
            'test_cases': test_case_results,
            'memory_usage': memory_info,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë√°nh gi√° model {model_name}: {e}")
        return None

def save_model_report(model_result: dict, reports_dir: Path):
    """L∆∞u b√°o c√°o cho m·ªôt model"""
    if not model_result:
        return
    
    model_name = model_result['model_name']
    safe_name = model_name.replace('/', '_').replace(':', '_')
    
    # Save detailed JSON report
    json_path = reports_dir / f"{safe_name}_detailed.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(model_result, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ƒê√£ l∆∞u b√°o c√°o chi ti·∫øt: {json_path}")
    
    # Save summary CSV
    summary_data = {
        'Model': model_name,
        'Dimension': model_result['embedding_dimension'],
        'Time(s)': model_result['evaluation_time_seconds'],
        **model_result['summary_metrics']
    }
    
    csv_path = reports_dir / f"{safe_name}_summary.csv"
    pd.DataFrame([summary_data]).to_csv(csv_path, index=False)
    
    return json_path

def create_visualizations(all_results: dict, visualizer: ResultVisualizer):
    """T·∫°o t·∫•t c·∫£ c√°c visualizations"""
    print("\nüìà ƒêang t·∫°o visualizations...")
    
    try:
        # Model comparison chart
        calculator = MetricsCalculator()
        model_metrics = {name: result['summary_metrics'] 
                        for name, result in all_results.items() 
                        if result is not None}
        
        if len(model_metrics) > 1:
            df_comparison = calculator.compare_models(model_metrics)
            
            # Save comparison plot
            comparison_path = visualizer.output_dir / "visualizations" / "model_comparison.png"
            visualizer.plot_model_comparison(df_comparison, str(comparison_path))
            
            # Create performance heatmap
            heatmap_path = visualizer.output_dir / "visualizations" / "performance_heatmap.png"
            visualizer.create_performance_heatmap(all_results, str(heatmap_path))
            
            # Save comparison data
            comparison_csv = visualizer.output_dir / "model_comparison.csv"
            df_comparison.to_csv(comparison_csv, index=False)
            print(f"üíæ ƒê√£ l∆∞u b·∫£ng so s√°nh: {comparison_csv}")
        
        # Individual model analysis
        for model_name, result in all_results.items():
            if result is not None:
                detailed_df = pd.DataFrame(result['detailed_metrics'])
                safe_name = model_name.replace('/', '_').replace(':', '_')
                
                detail_path = visualizer.output_dir / "visualizations" / f"{safe_name}_analysis.png"
                visualizer.plot_detailed_analysis(
                    model_name, detailed_df, result['summary_metrics'], str(detail_path)
                )
        
        # Generate HTML report
        visualizer.generate_performance_report(all_results)
        
        print("‚úÖ ƒê√£ t·∫°o xong t·∫•t c·∫£ visualizations")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o visualizations: {e}")

def main():
    """Main evaluation pipeline"""
    print("=" * 70)
    print("üáªüá≥ VIETNAMESE EMBEDDING MODELS EVALUATION PIPELINE")
    print("=" * 70)
    
    # Setup
    setup_directories()
    models_to_test, test_suite = load_configuration()
    
    if not models_to_test or not test_suite:
        print("‚ùå Kh√¥ng th·ªÉ load c·∫•u h√¨nh. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return
    
    # Process data
    print("\nüìö ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
    processor = DataProcessor()
    chunks = processor.load_and_process_content('data/content.md')
    
    if not chunks:
        print("‚ùå Kh√¥ng th·ªÉ load content data. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return
    
    chunk_texts = [chunk['text'] for chunk in chunks]
    chunk_ids = [chunk['id'] for chunk in chunks]
    
    # Save chunks info for debugging
    chunks_info_path = Path("reports") / "chunks_info.json"
    processor.save_chunks_info(chunks, chunks_info_path)
    
    # Initialize components
    reports_dir = Path("reports")
    visualizer = ResultVisualizer(str(reports_dir))
    all_results = {}
    
    # Evaluate each model
    print(f"\nüîÑ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° {len(models_to_test)} models...")
    
    for i, model_name in enumerate(models_to_test, 1):
        print(f"\n{'='*50}")
        print(f"üìä Model {i}/{len(models_to_test)}: {model_name}")
        print(f"{'='*50}")
        
        try:
            # Evaluate model
            result = evaluate_single_model(
                model_name, chunks, test_suite, chunk_texts, chunk_ids
            )
            
            if result:
                all_results[model_name] = result
                save_model_report(result, reports_dir)
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu d·ª´ng. ƒêang l∆∞u k·∫øt qu·∫£ hi·ªán t·∫°i
```python
            print("\n‚ö†Ô∏è  Nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu d·ª´ng. ƒêang l∆∞u k·∫øt qu·∫£ hi·ªán t·∫°i...")
            break
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng mong mu·ªën v·ªõi model {model_name}: {e}")
            continue
    
    # Create visualizations and final reports
    if all_results:
        create_visualizations(all_results, visualizer)
        
        # Save consolidated results
        consolidated_path = reports_dir / "all_results_consolidated.json"
        with open(consolidated_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p: {consolidated_path}")
        
        # Print final summary
        print("\n" + "="*70)
        print("üìã T·ªîNG K·∫æT K·ªÄT QU·∫¢ ƒê√ÅNH GI√Å")
        print("="*70)
        
        # Sort by MRR
        sorted_results = sorted(
            [(name, result['summary_metrics']['MRR']) for name, result in all_results.items()],
            key=lambda x: x[1], reverse=True
        )
        
        print("üèÜ B·∫¢NG X·∫æP H·∫†NG (theo MRR):")
        for rank, (model_name, mrr) in enumerate(sorted_results, 1):
            result = all_results[model_name]
            hit_1 = result['summary_metrics']['Hit_Rate@1']
            time_taken = result['evaluation_time_seconds']
            
            print(f"{rank:2d}. {model_name.split('/')[-1]:<40} "
                  f"MRR: {mrr:.4f} | Hit@1: {hit_1:.2%} | Time: {time_taken:.1f}s")
        
        best_model = sorted_results[0][0]
        best_metrics = all_results[best_model]['summary_metrics']
        
        print(f"\nü•á CHAMPION: {best_model}")
        print(f"   üìä MRR: {best_metrics['MRR']:.4f}")
        print(f"   üéØ Hit Rate@1: {best_metrics['Hit_Rate@1']:.2%}")
        print(f"   üìà Hit Rate@5: {best_metrics['Hit_Rate@5']:.2%}")
        
        print(f"\nüìÅ T·∫•t c·∫£ b√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c: {reports_dir.absolute()}")
        print("üìä Ki·ªÉm tra file 'performance_report.html' ƒë·ªÉ xem b√°o c√°o chi ti·∫øt")
        
    else:
        print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh v√† d·ªØ li·ªáu.")
    
    print("\nüéâ Ho√†n th√†nh pipeline ƒë√°nh gi√°!")
    print("="*70)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Ch∆∞∆°ng tr√¨nh ƒë√£ b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng.")
    except Exception as e:
        print(f"\n‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")
        import traceback
        traceback.print_exc()
```

## 11. README.md

```markdown
# Vietnamese Embedding Models Evaluation Pipeline üáªüá≥

M·ªôt pipeline t·ª± ƒë·ªông ƒë·ªÉ ƒë√°nh gi√° v√† so s√°nh hi·ªáu su·∫•t c·ªßa c√°c model embedding m√£ ngu·ªìn m·ªü tr√™n c√°c t√°c v·ª• t√¨m ki·∫øm ng·ªØ nghƒ©a (semantic search) cho ti·∫øng Vi·ªát.

## ‚ú® T√≠nh nƒÉng

- **ƒê√°nh gi√° t·ª± ƒë·ªông**: So s√°nh nhi·ªÅu models embedding c√πng l√∫c
- **Metrics to√†n di·ªán**: MRR, Hit Rate@k, MAP, NDCG, Precision, Recall
- **T·ªëi ∆∞u GPU**: H·ªó tr·ª£ CUDA v·ªõi qu·∫£n l√Ω memory th√¥ng minh
- **Visualization**: Bi·ªÉu ƒë·ªì v√† b√°o c√°o HTML chi ti·∫øt
- **Ti·∫øng Vi·ªát**: T·ªëi ∆∞u cho vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi pyvi v√† underthesea

## üöÄ C√†i ƒë·∫∑t

### 1. Clone repository
```bash
git clone <your-repo>
cd vietnamese_embedding_evaluator
```

### 2. T·∫°o virtual environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ho·∫∑c
venv\Scripts\activate     # Windows
```

### 3. C√†i ƒë·∫∑t dependencies
```bash
pip install -r requirements.txt
```

### 4. C√†i ƒë·∫∑t th√™m cho GPU (n·∫øu c√≥)
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## üìä C√°ch s·ª≠ d·ª•ng

### Ch·∫°y evaluation pipeline
```bash
python evaluate.py
```

### C·∫•u tr√∫c th∆∞ m·ª•c sau khi ch·∫°y
```
vietnamese_embedding_evaluator/
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.csv              # B·∫£ng so s√°nh c√°c models
‚îÇ   ‚îú‚îÄ‚îÄ performance_report.html          # B√°o c√°o HTML chi ti·∫øt
‚îÇ   ‚îú‚îÄ‚îÄ all_results_consolidated.json    # K·∫øt qu·∫£ t·ªïng h·ª£p
‚îÇ   ‚îú‚îÄ‚îÄ chunks_info.json                 # Th√¥ng tin c√°c text chunks
‚îÇ   ‚îú‚îÄ‚îÄ visualizations/                  # Th∆∞ m·ª•c ch·ª©a c√°c bi·ªÉu ƒë·ªì
‚îÇ   ‚îî‚îÄ‚îÄ *_detailed.json                  # B√°o c√°o chi ti·∫øt t·ª´ng model
‚îî‚îÄ‚îÄ model_cache/                         # Cache c√°c models ƒë√£ t·∫£i
```

## üéØ Metrics ƒë∆∞·ª£c ƒë√°nh gi√°

- **MRR (Mean Reciprocal Rank)**: Ch·ªâ s·ªë ch√≠nh ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ranking
- **Hit Rate@k**: T·ª∑ l·ªá c√¢u h·ªèi c√≥ ƒë√°p √°n ƒë√∫ng trong top-k (k=1,3,5)
- **MAP@5**: Mean Average Precision t·∫°i top-5
- **NDCG@5**: Normalized Discounted Cumulative Gain t·∫°i top-5
- **Precision@5 & Recall@5**: ƒê·ªô ch√≠nh x√°c v√† ƒë·ªô bao ph·ªß

## üîß T√πy ch·ªânh

### Th√™m models m·ªõi
Ch·ªânh s·ª≠a `configs/models.json`:
```json
{
  "models": [
    "your-new-model/from-huggingface",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  ]
}
```

### Thay ƒë·ªïi d·ªØ li·ªáu test
- Ch·ªânh s·ª≠a `data/content.md` v·ªõi n·ªôi dung m·ªõi
- C·∫≠p nh·∫≠t `data/test_suite.json` v·ªõi c√¢u h·ªèi t∆∞∆°ng ·ª©ng

### ƒêi·ªÅu ch·ªânh chunk size
Trong `src/data_processor.py`, thay ƒë·ªïi:
```python
self.chunk_size = 150  # S·ªë t·ª´ t·ªëi ƒëa trong m·ªôt chunk
self.overlap = 30      # S·ªë t·ª´ overlap gi·ªØa c√°c chunk
```

## üìà K·∫øt qu·∫£ m·∫´u

```
üèÜ B·∫¢NG X·∫æP H·∫†NG (theo MRR):
 1. paraphrase-multilingual-mpnet-base-v2     MRR: 0.7234 | Hit@1: 62.50% | Time: 45.2s
 2. LaBSE                                     MRR: 0.6891 | Hit@1: 56.25% | Time: 52.1s
 3. vietnamese-sbert                          MRR: 0.6456 | Hit@1: 50.00% | Time: 38.7s
```

## üõ†Ô∏è Troubleshooting

### L·ªói CUDA out of memory
- Gi·∫£m batch_size trong `embedding_manager.py`
- ƒê√°nh gi√° t·ª´ng model m·ªôt: comment b·ªõt models trong `configs/models.json`

### L·ªói t√°ch t·ª´ ti·∫øng Vi·ªát
```bash
# C√†i ƒë·∫∑t l·∫°i underthesea
pip uninstall underthesea
pip install underthesea
```

### Model kh√¥ng t·∫£i ƒë∆∞·ª£c
- Ki·ªÉm tra k·∫øt n·ªëi internet
- X√≥a `model_cache/` v√† ch·∫°y l·∫°i

## ü§ù ƒê√≥ng g√≥p

1. Fork repository
2. T·∫°o feature branch: `git checkout -b feature/new-feature`
3. Commit changes: `git commit -m 'Add new feature'`
4. Push: `git push origin feature/new-feature`
5. T·∫°o Pull Request

## üìù License

MIT License - xem file LICENSE ƒë·ªÉ bi·∫øt chi ti·∫øt.

## üôè Acknowledgments

- [Sentence Transformers](https://www.sbert.net/)
- [underthesea](https://github.com/undertheseanlp/underthesea)
- [pyvi](https://github.com/trungtv/pyvi)
- C·ªông ƒë·ªìng NLP Vi·ªát Nam
```

## 12. C√°ch ch·∫°y d·ª± √°n

1. **T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c** theo ƒë√∫ng layout tr√™n
2. **C√†i ƒë·∫∑t dependencies**:
```bash
pip install -r requirements.txt
```

3. **Ch·∫°y evaluation**:
```bash
python evaluate.py
```

## ƒê·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t c·ªßa d·ª± √°n:

‚úÖ **T·ªëi ∆∞u GPU**: T·ª± ƒë·ªông detect v√† s·ª≠ d·ª•ng GPU, qu·∫£n l√Ω memory th√¥ng minh  
‚úÖ **X·ª≠ l√Ω ti·∫øng Vi·ªát**: S·ª≠ d·ª•ng pyvi + underthesea ƒë·ªÉ t√°ch t·ª´ ch√≠nh x√°c  
‚úÖ **Metrics ƒë·∫ßy ƒë·ªß**: 8 metrics kh√°c nhau ƒë·ªÉ ƒë√°nh gi√° to√†n di·ªán  
‚úÖ **Visualizations phong ph√∫**: Bi·ªÉu ƒë·ªì, heatmap, b√°o c√°o HTML  
‚úÖ **Error handling**: X·ª≠ l√Ω l·ªói t·ªët, c√≥ th·ªÉ ti·∫øp t·ª•c khi m·ªôt model l·ªói  
‚úÖ **Performance monitoring**: Theo d√µi th·ªùi gian, memory usage  
‚úÖ **Extensible**: D·ªÖ d√†ng th√™m models, metrics, visualization m·ªõi  

D·ª± √°n n√†y s·∫Ω gi√∫p b·∫°n ƒë√°nh gi√° m·ªôt c√°ch kh√°ch quan v√† to√†n di·ªán c√°c model embedding ti·∫øng Vi·ªát, v·ªõi giao di·ªán ng∆∞·ªùi d√πng th√¢n thi·ªán v√† b√°o c√°o chi ti·∫øt!

