import os
import json
import pandas as pd
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import numpy as np
from pyvi import ViTokenizer
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import torch
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

from src.data_processor import DataProcessor
from src.embedding_manager import EmbeddingManager
from src.metrics import MetricsCalculator
from src.visualizer import ResultVisualizer

def print_banner():
    """In banner ch√†o m·ª´ng"""
    banner = """
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë                  üáªüá≥ VIETNAMESE EMBEDDING EVALUATOR üáªüá≥                ‚ïë
    ‚ïë                                                                      ‚ïë
    ‚ïë              ƒê√°nh gi√° hi·ªáu su·∫•t c√°c model embedding ti·∫øng Vi·ªát       ‚ïë
    ‚ïë                         Phi√™n b·∫£n 1.0 - 2024                        ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)

def setup_directories():
    """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    directories = [
        'reports', 
        'reports/visualizations', 
        'reports/individual_models',
        'model_cache', 
        'logs'
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    print("üìÅ ƒê√£ t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c")

def check_gpu_availability():
    """Ki·ªÉm tra t√¨nh tr·∫°ng GPU"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"üöÄ GPU kh·∫£ d·ª•ng: {gpu_name} ({gpu_memory:.1f} GB VRAM)")
        return True
    else:
        print("üíª Ch·ªâ c√≥ CPU kh·∫£ d·ª•ng (kh√¥ng c√≥ GPU)")
        return False

def load_configuration():
    """Load c·∫•u h√¨nh models v√† test suite"""
    print("\nüîß ƒêang load c·∫•u h√¨nh...")
    
    # Load models list
    try:
        config_path = Path('configs/models.json')
        if not config_path.exists():
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {config_path}")
            return None, None
            
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        models_to_test = config.get('models', [])
        if not models_to_test:
            print("‚ùå Kh√¥ng c√≥ models n√†o trong config")
            return None, None
            
        print(f"‚úÖ ƒê√£ load {len(models_to_test)} models ƒë·ªÉ test:")
        for i, model in enumerate(models_to_test, 1):
            print(f"   {i}. {model}")
            
    except Exception as e:
        print(f"‚ùå L·ªói khi load models config: {e}")
        return None, None
    
    # Load test suite
    try:
        test_suite_path = Path('data/test_suite.json')
        if not test_suite_path.exists():
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {test_suite_path}")
            return None, None
            
        with open(test_suite_path, 'r', encoding='utf-8') as f:
            test_suite = json.load(f)
            
        if not test_suite:
            print("‚ùå Test suite tr·ªëng")
            return None, None
            
        print(f"‚úÖ ƒê√£ load {len(test_suite)} c√¢u h·ªèi test")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi load test suite: {e}")
        return None, None
    
    return models_to_test, test_suite

def process_content_data():
    """X·ª≠ l√Ω d·ªØ li·ªáu content"""
    print("\nüìö ƒêang x·ª≠ l√Ω d·ªØ li·ªáu content...")
    
    content_path = Path('data/content.md')
    if not content_path.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {content_path}")
        return None
    
    processor = DataProcessor()
    
    # Th·ª≠ c√°c ph∆∞∆°ng ph√°p chunking kh√°c nhau
    chunk_methods = ['sentences', 'words']
    best_chunks = None
    
    for method in chunk_methods:
        print(f"\nüìù Th·ª≠ ph∆∞∆°ng ph√°p chunking: {method}")
        try:
            chunks = processor.load_and_process_content(str(content_path), chunk_method=method)
            if chunks:
                best_chunks = chunks
                print(f"‚úÖ S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p: {method}")
                break
        except Exception as e:
            print(f"‚ö†Ô∏è  Ph∆∞∆°ng ph√°p {method} g·∫∑p l·ªói: {e}")
            continue
    
    if not best_chunks:
        print("‚ùå Kh√¥ng th·ªÉ t·∫°o chunks b·∫±ng b·∫•t k·ª≥ ph∆∞∆°ng ph√°p n√†o")
        return None
    
    # L∆∞u th√¥ng tin chunks
    chunks_info_path = Path("reports") / "chunks_info.json"
    processor.save_chunks_info(best_chunks, str(chunks_info_path))
    
    return best_chunks, processor

def evaluate_single_model(model_name: str, chunks: list, test_suite: list, 
                         chunk_texts: list, chunk_ids: list, processor: DataProcessor) -> dict:
    """ƒê√°nh gi√° m·ªôt model duy nh·∫•t"""
    print(f"\n{'='*60}")
    print(f"üöÄ ƒêang ƒë√°nh gi√° model: {model_name}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        # Kh·ªüi t·∫°o embedding manager
        print("üì• ƒêang t·∫£i model...")
        manager = EmbeddingManager(model_name, cache_dir="model_cache")
        model_info = manager.get_model_info()
        
        print(f"‚ÑπÔ∏è  Th√¥ng tin model:")
        print(f"   - T√™n: {model_info['name']}")
        print(f"   - Dimension: {model_info['dimension']}")
        print(f"   - Device: {model_info['device']}")
        print(f"   - Max sequence length: {model_info['max_sequence_length']}")
        
        # Validate test suite v·ªõi chunks
        is_valid = processor.validate_test_suite(test_suite, chunk_ids)
        if not is_valid:
            print("‚ö†Ô∏è  Test suite c√≥ v·∫•n ƒë·ªÅ, nh∆∞ng s·∫Ω ti·∫øp t·ª•c...")
        
        # Encode corpus
        print(f"\nüìù ƒêang encode {len(chunk_texts)} chunks...")
        corpus_embeddings = manager.encode_texts(
            chunk_texts, 
            batch_size=32, 
            show_progress=True
        )
        
        print(f"‚úÖ ƒê√£ encode corpus: {corpus_embeddings.shape}")
        
        # Process test cases
        print(f"\nüîç ƒêang x·ª≠ l√Ω {len(test_suite)} c√¢u h·ªèi test...")
        test_case_results = []
        
        questions = [test_case['question'] for test_case in test_suite]
        
        # Batch encode questions
        print("üìù ƒêang encode c√°c c√¢u h·ªèi...")
        question_embeddings = manager.encode_texts(questions, show_progress=False)
        
        # Process each question
        for i, (test_case, query_embedding) in enumerate(zip(test_suite, question_embeddings)):
            question = test_case['question']
            correct_chunk_id = test_case['correct_chunk_id']
            
            # Find similar chunks
            top_indices, top_scores = manager.find_most_similar(
                query_embedding.unsqueeze(0), 
                corpus_embeddings, 
                top_k=5
            )
            
            # Build results
            top_results = []
            for rank, (idx, score) in enumerate(zip(top_indices, top_scores), 1):
                chunk_text = chunks[idx]['text']
                top_results.append({
                    'chunk_id': chunk_ids[idx],
                    'score': float(score),
                    'rank': rank,
                    'text_preview': chunk_text[:150] + "..." if len(chunk_text) > 150 else chunk_text
                })
            
            # Find rank of correct answer
            found_rank = None
            for result in top_results:
                if result['chunk_id'] == correct_chunk_id:
                    found_rank = result['rank']
                    break
            
            test_case_results.append({
                'question': question,
                'correct_chunk_id': correct_chunk_id,
                'found_rank': found_rank,
                'top_5_results': top_results
            })
            
            # Progress update
            if (i + 1) % 5 == 0 or (i + 1) == len(test_suite):
                print(f"  ‚úì ƒê√£ x·ª≠ l√Ω {i + 1}/{len(test_suite)} c√¢u h·ªèi")
        
        # Calculate metrics
        print("\nüìä ƒêang t√≠nh to√°n metrics...")
        calculator = MetricsCalculator()
        summary_metrics, detailed_df = calculator.calculate_detailed_metrics(test_case_results)
        performance_analysis = calculator.get_performance_analysis(detailed_df)
        
        # Memory info
        memory_info = manager.get_memory_usage()
        
        evaluation_time = time.time() - start_time
        
        # Print summary
        print(f"\nüìà K·∫øt qu·∫£ ƒë√°nh gi√°:")
        print(f"   ‚è±Ô∏è  Th·ªùi gian: {evaluation_time:.2f}s")
        print(f"   üéØ MRR: {summary_metrics['MRR']:.4f}")
        print(f"   ü•á Hit Rate@1: {summary_metrics['Hit_Rate@1']:.2%}")
        print(f"   ü•â Hit Rate@3: {summary_metrics['Hit_Rate@3']:.2%}")
        print(f"   üèÖ Hit Rate@5: {summary_metrics['Hit_Rate@5']:.2%}")
        
        # Clean up GPU memory
        manager.clear_cache()
        del manager
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return {
            'model_name': model_name,
            'model_info': model_info,
            'embedding_dimension': model_info['dimension'],
            'evaluation_time_seconds': evaluation_time,
            'summary_metrics': summary_metrics,
            'detailed_metrics': detailed_df.to_dict('records'),
            'performance_analysis': performance_analysis,
            'test_cases': test_case_results,
            'memory_usage': memory_info,
            'timestamp': datetime.now().isoformat(),
            'total_questions': len(test_suite),
            'total_chunks': len(chunks)
        }
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë√°nh gi√° model {model_name}: {e}")
        import traceback
        print("üîç Chi ti·∫øt l·ªói:")
        traceback.print_exc()
        return None

def save_model_report(model_result: dict, reports_dir: Path) -> Path:
    """L∆∞u b√°o c√°o chi ti·∫øt cho m·ªôt model"""
    if not model_result:
        return None
    
    model_name = model_result['model_name']
    safe_name = model_name.replace('/', '_').replace(':', '_').replace('-', '_')
    
    # T·∫°o th∆∞ m·ª•c ri√™ng cho model
    model_dir = reports_dir / "individual_models" / safe_name
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # Save detailed JSON report
    json_path = model_dir / f"{safe_name}_full_report.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(model_result, f, ensure_ascii=False, indent=2)
    
    # Save summary report
    summary_data = {
        'Model': model_name,
        'Dimension': model_result['embedding_dimension'],
        'Evaluation_Time_Seconds': model_result['evaluation_time_seconds'],
        'Total_Questions': model_result['total_questions'],
        'Total_Chunks': model_result['total_chunks'],
        **model_result['summary_metrics'],
        'Timestamp': model_result['timestamp']
    }
    
    summary_path = model_dir / f"{safe_name}_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    # Save detailed results as CSV
    detailed_df = pd.DataFrame(model_result['detailed_metrics'])
    csv_path = model_dir / f"{safe_name}_detailed_results.csv"
    detailed_df.to_csv(csv_path, index=False, encoding='utf-8')
    
    print(f"üíæ ƒê√£ l∆∞u b√°o c√°o model t·∫°i: {model_dir}")
    
    return json_path

def create_overall_summary(all_results: dict, reports_dir: Path):
    """T·∫°o b√°o c√°o t·ªïng h·ª£p t·∫•t c·∫£ models"""
    print("\nüìã ƒêang t·∫°o b√°o c√°o t·ªïng h·ª£p...")
    
    # Prepare summary data
    summary_data = []
    for model_name, result in all_results.items():
        if result is not None:
            row = {
                'Model': model_name,
                'Short_Name': model_name.split('/')[-1],
                'Dimension': result['embedding_dimension'],
                'Time_Seconds': result['evaluation_time_seconds'],
                'Questions': result['total_questions'],
                'Chunks': result['total_chunks'],
                **result['summary_metrics']
            }
            summary_data.append(row)
    
    if not summary_data:
        print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë·ªÉ t·∫°o b√°o c√°o t·ªïng h·ª£p")
        return
    
    # Create DataFrame and sort by MRR
    df_summary = pd.DataFrame(summary_data)
    df_summary = df_summary.sort_values('MRR', ascending=False).reset_index(drop=True)
    df_summary['Rank'] = df_summary.index + 1
    
    # Save CSV
    summary_csv_path = reports_dir / "overall_summary.csv"
    df_summary.to_csv(summary_csv_path, index=False, encoding='utf-8')
    
    # Save JSON
    summary_json_path = reports_dir / "overall_summary.json"
    with open(summary_json_path, 'w', encoding='utf-8') as f:
        json.dump(df_summary.to_dict('records'), f, ensure_ascii=False, indent=2)
    
    # Print summary table
    print("\nüèÜ BXH MODELS (S·∫Øp x·∫øp theo MRR):")
    print("=" * 100)
    print(f"{'Rank':<4} {'Model':<40} {'MRR':<8} {'Hit@1':<8} {'Hit@5':<8} {'Time':<8}")
    print("=" * 100)
    
    for _, row in df_summary.iterrows():
        print(f"{row['Rank']:<4} {row['Short_Name']:<40} "
              f"{row['MRR']:<8.4f} {row['Hit_Rate@1']:<8.2%} "
              f"{row['Hit_Rate@5']:<8.2%} {row['Time_Seconds']:<8.1f}")
    
    print("=" * 100)
    print(f"üíæ ƒê√£ l∆∞u b√°o c√°o t·ªïng h·ª£p: {summary_csv_path}")
    
    return df_summary

def create_visualizations(all_results: dict, visualizer: ResultVisualizer):
    """T·∫°o t·∫•t c·∫£ c√°c visualizations"""
    print("\nüìà ƒêang t·∫°o visualizations...")
    
    try:
        # Model comparison chart
        calculator = MetricsCalculator()
        model_metrics = {name: result['summary_metrics'] 
                        for name, result in all_results.items() 
                        if result is not None}
        
        if len(model_metrics) > 1:
            df_comparison = calculator.compare_models(model_metrics)
            
            # Save comparison plot
            comparison_path = visualizer.output_dir / "visualizations" / "model_comparison.png"
            visualizer.plot_model_comparison(df_comparison, str(comparison_path))
            
            # Create performance heatmap
            heatmap_path = visualizer.output_dir / "visualizations" / "performance_heatmap.png"
            visualizer.create_performance_heatmap(all_results, str(heatmap_path))
            
            # Save comparison data
            comparison_csv = visualizer.output_dir / "model_comparison.csv"
            df_comparison.to_csv(comparison_csv, index=False)
            print(f"üíæ ƒê√£ l∆∞u b·∫£ng so s√°nh: {comparison_csv}")
        
        # Individual model analysis
        for model_name, result in all_results.items():
            if result is not None:
                detailed_df = pd.DataFrame(result['detailed_metrics'])
                safe_name = model_name.replace('/', '_').replace(':', '_')
                
                detail_path = visualizer.output_dir / "visualizations" / f"{safe_name}_analysis.png"
                visualizer.plot_detailed_analysis(
                    model_name, detailed_df, result['summary_metrics'], str(detail_path)
                )
        
        # Generate HTML report
        visualizer.generate_performance_report(all_results)
        
        print("‚úÖ ƒê√£ t·∫°o xong t·∫•t c·∫£ visualizations")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o visualizations: {e}")

def main():
    """Main evaluation pipeline"""
    print("=" * 70)
    print("üáªüá≥ VIETNAMESE EMBEDDING MODELS EVALUATION PIPELINE")
    print("=" * 70)
    
    # Setup
    setup_directories()
    models_to_test, test_suite = load_configuration()
    
    if not models_to_test or not test_suite:
        print("‚ùå Kh√¥ng th·ªÉ load c·∫•u h√¨nh. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return
    
    # Process data
    print("\nüìö ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
    processor = DataProcessor()
    chunks = processor.load_and_process_content('data/content.md')
    
    if not chunks:
        print("‚ùå Kh√¥ng th·ªÉ load content data. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return
    
    chunk_texts = [chunk['text'] for chunk in chunks]
    chunk_ids = [chunk['id'] for chunk in chunks]
    
    # Save chunks info for debugging
    chunks_info_path = Path("reports") / "chunks_info.json"
    processor.save_chunks_info(chunks, chunks_info_path)
    
    # Initialize components
    reports_dir = Path("reports")
    visualizer = ResultVisualizer(str(reports_dir))
    all_results = {}
    
    # Evaluate each model
    print(f"\nüîÑ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° {len(models_to_test)} models...")
    
    for i, model_name in enumerate(models_to_test, 1):
        print(f"\n{'='*50}")
        print(f"üìä Model {i}/{len(models_to_test)}: {model_name}")
        print(f"{'='*50}")
        
        try:
            # Evaluate model
            result = evaluate_single_model(
                model_name, chunks, test_suite, chunk_texts, chunk_ids, processor
            )
            
            if result:
                all_results[model_name] = result
                save_model_report(result, reports_dir)
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu d·ª´ng. ƒêang l∆∞u k·∫øt qu·∫£ hi·ªán t·∫°i...")
            break
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng mong mu·ªën v·ªõi model {model_name}: {e}")
            continue
    
    # Create visualizations and final reports
    if all_results:
        create_visualizations(all_results, visualizer)
        
        # Save consolidated results
        consolidated_path = reports_dir / "all_results_consolidated.json"
        with open(consolidated_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p: {consolidated_path}")
        
        # Print final summary
        print("\n" + "="*70)
        print("üìã T·ªîNG K·∫æT K·ªÄT QU·∫¢ ƒê√ÅNH GI√Å")
        print("="*70)
        
        # Sort by MRR
        sorted_results = sorted(
            [(name, result['summary_metrics']['MRR']) for name, result in all_results.items()],
            key=lambda x: x[1], reverse=True
        )
        
        print("üèÜ B·∫¢NG X·∫æP H·∫†NG (theo MRR):")
        for rank, (model_name, mrr) in enumerate(sorted_results, 1):
            result = all_results[model_name]
            hit_1 = result['summary_metrics']['Hit_Rate@1']
            time_taken = result['evaluation_time_seconds']
            
            print(f"{rank:2d}. {model_name.split('/')[-1]:<40} "
                  f"MRR: {mrr:.4f} | Hit@1: {hit_1:.2%} | Time: {time_taken:.1f}s")
        
        best_model = sorted_results[0][0]
        best_metrics = all_results[best_model]['summary_metrics']
        
        print(f"\nü•á CHAMPION: {best_model}")
        print(f"   üìä MRR: {best_metrics['MRR']:.4f}")
        print(f"   üéØ Hit Rate@1: {best_metrics['Hit_Rate@1']:.2%}")
        print(f"   üìà Hit Rate@5: {best_metrics['Hit_Rate@5']:.2%}")
        
        print(f"\nüìÅ T·∫•t c·∫£ b√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c: {reports_dir.absolute()}")
        print("üìä Ki·ªÉm tra file 'performance_report.html' ƒë·ªÉ xem b√°o c√°o chi ti·∫øt")
        
    else:
        print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh v√† d·ªØ li·ªáu.")
    
    print("\nüéâ Ho√†n th√†nh pipeline ƒë√°nh gi√°!")
    print("="*70)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Ch∆∞∆°ng tr√¨nh ƒë√£ b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng.")
    except Exception as e:
        print(f"\n‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")
        import traceback
        traceback.print_exc()