import os
import re
import json
from pathlib import Path
from pyvi import ViTokenizer
from typing import List, Dict
import unicodedata

class DataProcessor:
    def __init__(self):
        self.chunk_size = 150  # Sá»‘ tá»« tá»‘i Ä‘a trong má»™t chunk
        self.overlap = 30      # Sá»‘ tá»« overlap giá»¯a cÃ¡c chunk
    
    def clean_text(self, text: str) -> str:
        """LÃ m sáº¡ch vÄƒn báº£n tiáº¿ng Viá»‡t"""
        # Normalize unicode
        text = unicodedata.normalize('NFC', text)
        
        # XÃ³a markdown headers
        text = re.sub(r'#+\s*', '', text)
        
        # XÃ³a cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n thiáº¿t
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # XÃ³a cÃ¡c kÃ½ tá»± khÃ´ng pháº£i chá»¯ cÃ¡i, sá»‘, dáº¥u cÃ¢u tiáº¿ng Viá»‡t
        text = re.sub(r'[^\w\s\.,!?;:()\-"\'Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃÃŒá»ˆÄ¨á»ŠÃ“Ã’á»ŽÃ•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»žá» á»¢ÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä]', ' ', text)
        
        return text.strip()
    
    def simple_sentence_split(self, text: str) -> List[str]:
        """TÃ¡ch cÃ¢u Ä‘Æ¡n giáº£n cho tiáº¿ng Viá»‡t"""
        # CÃ¡c dáº¥u káº¿t thÃºc cÃ¢u
        sentence_endings = r'[.!?]\s+'
        
        # TÃ¡ch cÃ¢u
        sentences = re.split(sentence_endings, text)
        
        # Loáº¡i bá» cÃ¢u rá»—ng vÃ  cÃ¢u quÃ¡ ngáº¯n
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        return sentences
    
    def segment_vietnamese_text(self, text: str) -> List[str]:
        """TÃ¡ch tá»« tiáº¿ng Viá»‡t sá»­ dá»¥ng pyvi"""
        try:
            # TÃ¡ch cÃ¢u trÆ°á»›c
            sentences = self.simple_sentence_split(text)
            
            # TÃ¡ch tá»« cho má»—i cÃ¢u
            segmented_sentences = []
            for sentence in sentences:
                if sentence.strip():
                    segmented = ViTokenizer.tokenize(sentence.strip())
                    segmented_sentences.append(segmented)
            
            return segmented_sentences
        except Exception as e:
            print(f"Lá»—i khi tÃ¡ch tá»« vá»›i pyvi: {e}")
            # Fallback: chá»‰ tÃ¡ch cÃ¢u Ä‘Æ¡n giáº£n
            return self.simple_sentence_split(text)
    
    def create_chunks_by_sentences(self, text: str) -> List[Dict[str, str]]:
        """Chia vÄƒn báº£n thÃ nh chunks theo cÃ¢u"""
        cleaned_text = self.clean_text(text)
        sentences = self.segment_vietnamese_text(cleaned_text)
        
        chunks = []
        current_chunk_sentences = []
        current_word_count = 0
        chunk_counter = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            # Náº¿u thÃªm cÃ¢u nÃ y vÃ o chunk hiá»‡n táº¡i vÆ°á»£t quÃ¡ giá»›i háº¡n
            if current_word_count + sentence_word_count > self.chunk_size:
                if current_chunk_sentences:
                    # Táº¡o chunk tá»« cÃ¡c cÃ¢u hiá»‡n táº¡i
                    chunk_text = ' '.join(current_chunk_sentences)
                    chunks.append({
                        'id': f'chunk_{chunk_counter}',
                        'text': chunk_text,
                        'word_count': current_word_count,
                        'sentence_count': len(current_chunk_sentences)
                    })
                    chunk_counter += 1
                    
                    # Giá»¯ láº¡i má»™t vÃ i cÃ¢u cuá»‘i Ä‘á»ƒ táº¡o overlap
                    overlap_sentences = current_chunk_sentences[-2:] if len(current_chunk_sentences) > 2 else current_chunk_sentences
                    current_chunk_sentences = overlap_sentences + [sentence]
                    current_word_count = sum(len(s.split()) for s in current_chunk_sentences)
                else:
                    current_chunk_sentences = [sentence]
                    current_word_count = sentence_word_count
            else:
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
        
        # ThÃªm chunk cuá»‘i cÃ¹ng náº¿u cÃ³
        if current_chunk_sentences:
            chunk_text = ' '.join(current_chunk_sentences)
            chunks.append({
                'id': f'chunk_{chunk_counter}',
                'text': chunk_text,
                'word_count': current_word_count,
                'sentence_count': len(current_chunk_sentences)
            })
        
        return chunks
    
    def create_chunks_by_words(self, text: str) -> List[Dict[str, str]]:
        """Chia vÄƒn báº£n thÃ nh chunks theo tá»« (sliding window)"""
        cleaned_text = self.clean_text(text)
        
        # TÃ¡ch tá»« cho toÃ n bá»™ vÄƒn báº£n
        try:
            segmented_text = ViTokenizer.tokenize(cleaned_text)
            words = segmented_text.split()
        except Exception as e:
            print(f"Lá»—i khi tÃ¡ch tá»«: {e}. Sá»­ dá»¥ng tÃ¡ch tá»« Ä‘Æ¡n giáº£n.")
            words = cleaned_text.split()
        
        chunks = []
        chunk_counter = 0
        
        # Táº¡o chunks vá»›i sliding window
        start_idx = 0
        while start_idx < len(words):
            end_idx = min(start_idx + self.chunk_size, len(words))
            
            chunk_words = words[start_idx:end_idx]
            chunk_text = ' '.join(chunk_words)
            
            chunks.append({
                'id': f'chunk_{chunk_counter}',
                'text': chunk_text,
                'word_count': len(chunk_words),
                'start_word_idx': start_idx,
                'end_word_idx': end_idx
            })
            
            chunk_counter += 1
            
            # Di chuyá»ƒn window vá»›i overlap
            start_idx += (self.chunk_size - self.overlap)
            
            # Náº¿u chunk tiáº¿p theo sáº½ quÃ¡ ngáº¯n, dá»«ng láº¡i
            if len(words) - start_idx < self.overlap:
                break
        
        return chunks
    
    def create_chunks(self, text: str, method: str = 'sentences') -> List[Dict[str, str]]:
        """Chia vÄƒn báº£n thÃ nh chunks vá»›i phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c chá»n"""
        if method == 'sentences':
            return self.create_chunks_by_sentences(text)
        elif method == 'words':
            return self.create_chunks_by_words(text)
        else:
            raise ValueError(f"PhÆ°Æ¡ng phÃ¡p khÃ´ng há»— trá»£: {method}")
    
    def load_and_process_content(self, file_path: str, chunk_method: str = 'sentences') -> List[Dict[str, str]]:
        """Load vÃ  xá»­ lÃ½ file content"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"ÄÃ£ Ä‘á»c file {file_path} ({len(content)} kÃ½ tá»±)")
            
            chunks = self.create_chunks(content, method=chunk_method)
            print(f"ÄÃ£ táº¡o {len(chunks)} chunks báº±ng phÆ°Æ¡ng phÃ¡p '{chunk_method}'")
            
            # In thá»‘ng kÃª
            if chunks:
                word_counts = [chunk['word_count'] for chunk in chunks]
                print(f"Sá»‘ tá»« trung bÃ¬nh má»—i chunk: {sum(word_counts) / len(word_counts):.1f}")
                print(f"Chunk ngáº¯n nháº¥t: {min(word_counts)} tá»«")
                print(f"Chunk dÃ i nháº¥t: {max(word_counts)} tá»«")
                
                # In má»™t vÃ i chunk máº«u
                print("\n--- Chunk máº«u ---")
                for i in range(min(3, len(chunks))):
                    print(f"Chunk {i}: {chunks[i]['text'][:100]}...")
            
            return chunks
            
        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
            return []
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ file: {e}")
            return []
    
    def load_test_suite(self, file_path: str) -> List[Dict]:
        """Load bá»™ test questions"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                test_suite = json.load(f)
            print(f"âœ… ÄÃ£ load {len(test_suite)} cÃ¢u há»i test")
            
            # Kiá»ƒm tra format
            for i, test_case in enumerate(test_suite):
                if 'question' not in test_case or 'correct_chunk_id' not in test_case:
                    print(f"âš ï¸  Test case {i} thiáº¿u trÆ°á»ng báº¯t buá»™c")
            
            return test_suite
        except Exception as e:
            print(f"âŒ Lá»—i khi load test suite: {e}")
            return []
    
    def validate_test_suite(self, test_suite: List[Dict], chunk_ids: List[str]) -> bool:
        """Kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a test suite"""
        print("ðŸ” Äang kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a test suite...")
        
        valid = True
        missing_chunks = []
        
        for i, test_case in enumerate(test_suite):
            correct_id = test_case.get('correct_chunk_id')
            if correct_id not in chunk_ids:
                missing_chunks.append((i, correct_id))
                valid = False
        
        if missing_chunks:
            print(f"âš ï¸  TÃ¬m tháº¥y {len(missing_chunks)} test cases cÃ³ chunk_id khÃ´ng tá»“n táº¡i:")
            for i, chunk_id in missing_chunks[:5]:  # Hiá»ƒn thá»‹ 5 cÃ¡i Ä‘áº§u
                print(f"   Test case {i}: {chunk_id}")
            if len(missing_chunks) > 5:
                print(f"   ... vÃ  {len(missing_chunks) - 5} cases khÃ¡c")
        else:
            print("âœ… Test suite há»£p lá»‡")
        
        return valid
    
    def save_chunks_info(self, chunks: List[Dict], output_path: str):
        """LÆ°u thÃ´ng tin chunks Ä‘á»ƒ debug"""
        chunks_info = {
            'total_chunks': len(chunks),
            'avg_word_count': sum(c['word_count'] for c in chunks) / len(chunks) if chunks else 0,
            'chunks': chunks
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks_info, f, ensure_ascii=False, indent=2)
        
        print(f"ðŸ’¾ ÄÃ£ lÆ°u thÃ´ng tin chunks vÃ o: {output_path}")
    
    def create_balanced_test_suite(self, chunks: List[Dict], num_questions: int = 20) -> List[Dict]:
        """Táº¡o test suite cÃ¢n báº±ng tá»« chunks (Ä‘á»ƒ testing)"""
        if len(chunks) < num_questions:
            print(f"âš ï¸  Chá»‰ cÃ³ {len(chunks)} chunks, khÃ´ng thá»ƒ táº¡o {num_questions} cÃ¢u há»i")
            num_questions = len(chunks)
        
        # Chá»n chunks Ä‘áº¡i diá»‡n
        step = max(1, len(chunks) // num_questions)
        selected_chunks = chunks[::step][:num_questions]
        
        test_suite = []
        for i, chunk in enumerate(selected_chunks):
            # Táº¡o cÃ¢u há»i Ä‘Æ¡n giáº£n tá»« chunk
            text = chunk['text']
            sentences = self.simple_sentence_split(text)
            
            if sentences:
                # Láº¥y cÃ¢u Ä‘áº§u tiÃªn lÃ m base cho cÃ¢u há»i
                first_sentence = sentences[0]
                question = f"Ná»™i dung nÃ o nÃ³i vá» {first_sentence[:50]}...?"
                
                test_suite.append({
                    'question': question,
                    'correct_chunk_id': chunk['id']
                })
        
        return test_suite